name: OnnxRuntime CUDA Inference Tests

on:
  workflow_dispatch:
  pull_request:
    types: [opened, reopened, synchronize]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  build-and-test:
    runs-on: hf-dgx-01
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Build image
        run: |
          docker build
            --file docker/cuda.dockerfile
            --tag opt-bench-cuda:11.8.0-cudnn8
            --build-arg CUDA_VERSION=11.8.0
            --build-arg UBUNTU_VERSION=22.04
            .

      - name: Run tests
        run: |
          docker run
            --rm
            --gpus '"device=0,1"'
            --entrypoint /bin/bash
            --volume $(pwd):/workspace/optimum-benchmark
            --workdir /workspace/optimum-benchmark
            --env USE_CUDA="1"
            opt-bench-cuda:11.8.0-cudnn8
            -c "pip install -e .[test,onnxruntime-gpu,diffusers] && pytest -k 'cuda and onnxruntime and inference' -x"
