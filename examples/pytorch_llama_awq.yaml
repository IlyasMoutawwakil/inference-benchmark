defaults:
  - benchmark
  - scenario: inference
  - launcher: process
  - backend: pytorch
  - _self_

experiment_name: pytorch_llama_awq

launcher:
  device_isolation: true
  device_isolation_action: warn

backend:
  device: cuda
  device_ids: 0
  no_weights: true
  model: TheBloke/Llama-2-70B-AWQ

benchmark:
  input_shapes:
    batch_size: 1
    sequence_length: 128
  generate_kwargs:
    max_new_tokens: 100
    min_new_tokens: 100
