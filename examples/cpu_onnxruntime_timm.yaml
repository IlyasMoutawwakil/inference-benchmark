defaults:
  - benchmark
  - backend: onnxruntime
  - scenario: inference
  - launcher: process
  - _base_
  - _self_

name: onnxruntime_timm

backend:
  device: cpu
  export: true
  model: timm/tiny_vit_21m_224.in1k

scenario:
  memory: true
  latency: true
  input_shapes:
    batch_size: 2
