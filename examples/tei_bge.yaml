defaults:
  - benchmark
  - scenario: inference
  - launcher: inline
  - backend: py-txi
  - _base_
  - _self_

name: tei_bert

backend:
  device: cpu
  model: BAAI/bge-base-en-v1.5

scenario:
  input_shapes:
    batch_size: 64
    sequence_length: 128
