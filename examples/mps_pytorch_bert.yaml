defaults:
  - benchmark
  - scenario: inference
  - launcher: inline # mps fails with python multi-processing for some reason
  - backend: pytorch
  - _base_
  - _self_

name: mps_pytorch_bert

scenario:
  latency: true
  memory: true
  input_shapes:
    batch_size: 1
    sequence_length: 128

backend:
  device: mps
  no_weights: true
  model: bert-base-uncased
