defaults:
  - benchmark
  - scenario: inference
  - launcher: process
  - backend: ipex
  - _base_
  - _self_

name: cpu_ipex_llama

launcher:
  numactl: true
  numactl_kwargs:
    cpunodebind: 0
    membind: 0

backend:
  device: cpu
  export: true
  no_weights: false # because on multi-node machines, intializing weights could harm performance
  torch_dtype: float32 # but use bfloat16 on compatible Intel CPUs
  model: TinyLlama/TinyLlama-1.1B-Chat-v1.0

scenario:
  memory: true
  latency: true

  input_shapes:
    batch_size: 1
    sequence_length: 64

  generate_kwargs:
    max_new_tokens: 32
    min_new_tokens: 32
