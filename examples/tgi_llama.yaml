defaults:
  - benchmark
  - scenario: inference
  - launcher: inline
  - backend: py-txi
  - _base_
  - _self_

name: tgi_llama

backend:
  device: cuda
  device_ids: 0,1
  model: NousResearch/Nous-Hermes-llama-2-7b

scenario:
  input_shapes:
    batch_size: 4
    sequence_length: 256
  generate_kwargs:
    max_new_tokens: 100
    min_new_tokens: 100
