defaults:
  - benchmark
  - scenario: inference
  - launcher: inline
  - backend: llama_cpp
  - _base_
  - _self_

name: llama_cpp_llama

backend:
  device: mps
  model: nomic-ai/nomic-embed-text-v1.5-GGUF
  task: feature-extraction
  filename: nomic-embed-text-v1.5.Q4_0.gguf

scenario:
  input_shapes:
    batch_size: 1
    sequence_length: 256
    vocab_size: 30000
    type_vocab_size: 1
    max_position_embeddings: 512
  generate_kwargs:
    max_new_tokens: 100
    min_new_tokens: 100
