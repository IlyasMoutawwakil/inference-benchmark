# Distributed Data Parallel (DDP) training
experiment_name: ${device}_${backend.name}_${benchmark.name}_${task}_ddp

benchmark:
  dataset_shapes:
    dataset_size: 1600
    sequence_length: 256
  training_arguments:
    per_device_train_batch_size: 8

backend:
  use_ddp: true
  ddp_config:
    rdzv_endpoint: 127.0.0.1:29509

hydra:
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: 0,1
