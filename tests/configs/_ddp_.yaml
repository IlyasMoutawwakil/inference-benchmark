# Distributed Data Parallel (DDP) training
defaults:
  - override launcher: torchrun # for DDP training, use torchrun launcher

experiment_name: ${device}_${backend.name}_${benchmark.name}_${task}_ddp

launcher:
  nproc_per_node: 2
  rdzv_endpoint: "localhost:29222"

benchmark:
  dataset_shapes:
    dataset_size: 1600
    sequence_length: 256
  training_arguments:
    per_device_train_batch_size: 8

hydra:
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: 0,1
