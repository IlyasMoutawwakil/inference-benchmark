# Distributed Data Parallel (DDP) training
defaults:
  - override launcher: torchrun

launcher:
  nproc_per_node: 2

hydra:
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: 0,1
