# Distributed Data Parallel (DDP) training
defaults:
  - launcher: torchrun

hydra:
  job:
    env_set:
      CUDA_VISIBLE_DEVICES: 0,1

experiment_name: ${device}_${backend.name}_${benchmark.name}_${task}_ddp

launcher:
  nproc_per_node: 2

benchmark:
  dataset_shapes:
    dataset_size: 1600
    sequence_length: 256
  training_arguments:
    per_device_train_batch_size: 8
